---
title: "ADNI Data V2 - Survival Analysis Predictive Models"
output: html_document
date: "2023-03-29"
---

# ADNI Data Version 5 - Survival Analysis Predictive Models - testing over specific hyperparameter values + trying one recipe over all models

## Install and load the relevant packages

```{r}

# install.packages("caret")
library(caret)

# install.packages("ranger")
library(ranger)

# install.packages("tidymodels")
library(tidymodels)

# install.packages("tidyverse")
library(tidyverse)

# install.packages("glmnet")
library(glmnet)

# install.packages("modeldatatoo")
library(modeldatatoo)

# install.packages("aorsf")
library(aorsf)

# install.packages("censored")
library(censored)

# install.packages("survival")
library(survival)

# install.packages("doParallel")
library(doParallel)

library(pROC) #for AUC calculation

# Install and load necessary packages
# install.packages("tidymodels")
library(tidymodels)

# install.packages("survival")
library(survival)

# install.packages("doParallel")
library(doParallel)

# Set up parallel processing
registerDoParallel()

# Load the randomForestSRC package
# install.packages("randomForestSRC")
# library(randomForestSRC)
```

# Import data set, Pre-process & set up cross validation

```{r}

# Import preprocessed data 
surv_df <- read.csv("preprocessed_data.csv") 
# View(surv_df)

# Create survival object
surv_df1 <- surv_df %>% 
  mutate(diagnosis_surv = Surv(VISCODE, event)) %>%
  select(diagnosis_surv, everything())
# View(surv_df1)

# Convert PTGENDER to numeric (0 for Male, 1 for Female)
surv_df1 <- surv_df %>%
  mutate(SEX = ifelse(SEX == "Male", 0, 1))

# Define resampling scheme: repeated k-fold cross-validation
set.seed(403)
repeated_folds <- vfold_cv(surv_df1, v = 5, repeats = 3) # 5-fold cross-validation

# colnames(surv_df1)

# List of different recipes based on specified feature combinations

# D = Demographics 
# F = Family History
# DF = Demographics + Family History 
# DGF = Demographics + Genetics + Family History 
# DGCF = Demographics + Genetics + Cognition + Family History
# DGC = Demographics + Genetics + Cognition

recipes_list <- list(
  D = recipe(diagnosis_surv ~ AGE + SEX + EDU, data = surv_df1),
  Fh = recipe(diagnosis_surv ~ FH, data = surv_df1),
  DFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + FH, data = surv_df1),
  DGFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + APOE4 + FH, data = surv_df1),
  DGCFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + APOE4 + MMSE + mPACCtrailsB + FH, data = surv_df1),
  DGC = recipe(diagnosis_surv ~ APOE4 + AGE + SEX + EDU + MMSE + mPACCtrailsB, data = surv_df1)
)

# Initialize list to store results for each recipe and model
results_list <- list()

# Define tuning grid for Cox model (regularization penalty)
cox_grid <- expand.grid(
  penalty = c(0.001 ,0.01, 0.1, 1, 10, 100) # Specific values for regularization penalty
)


# Define tuning grid for Random Forest
rf_grid <- expand.grid(
  mtry = c(2, 3, 4),  # Number of variables randomly sampled at each split
  min_n = c(1, 2, 4)  # Minimum number of samples in a terminal node
)


```

# TRAIN

```{r}

# Iterate through recipes and run models for each
for (i in seq_along(recipes_list)) {
  # Select the current recipe
  current_recipe <- recipes_list[[i]]
  
  # Define models: Weibull, Cox, Random Forest
  # Weibull Model
  survreg_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(survival_reg() %>%
                set_engine("survival") %>%
                set_mode("censored regression"))

  # Cox Proportional Hazards Model
  predictors=current_recipe$var_info %>% filter(role=="predictor") %>% pull(variable) %>% paste(collapse="+")
  coxnet_wflow <- workflow() %>%
     add_formula(as.formula(str_c("Surv(VISCODE, event) ~ ", predictors))) %>%
     add_model(proportional_hazards(penalty = tune(), mixture=0) %>% 
                 set_engine("glmnet") %>% 
                 set_mode("censored regression"))
  
  # Random Forest Workflow
  oblique_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(rand_forest(trees = 1000, 
                          mtry = tune(), 
                          min_n = tune()) %>%
                set_engine("aorsf") %>%
                set_mode("censored regression"))



  # Define evaluation metrics for survival analysis
  survival_metrics <- metric_set(brier_survival_integrated, 
                                 roc_auc_survival, 
                                 concordance_survival)
  
  # Set evaluation time points for metrics
  evaluation_time_points <- seq(0, 144, 6)

  # Cross-validation resampling (for all models)
  set.seed(1)
  survreg_res <- fit_resamples(survreg_wflow, 
                               resamples = repeated_folds, 
                               metrics = survival_metrics, 
                               eval_time = evaluation_time_points, 
                               control = control_resamples(save_pred = TRUE))
  

  coxnet_res <- tune_grid(coxnet_wflow, 
                          resamples = repeated_folds, 
                          grid = cox_grid, 
                          metrics = survival_metrics, 
                          eval_time = evaluation_time_points, 
                          control = control_grid(save_pred = TRUE))
  
  oblique_res <- tune_grid(oblique_wflow, 
                           resamples = repeated_folds, 
                           grid = rf_grid, 
                           metrics = survival_metrics, 
                           eval_time = evaluation_time_points, 
                           control = control_grid(save_pred = TRUE))
  
  # Store the results for each model
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_Weibull")]] <- survreg_res
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_Cox")]] <- coxnet_res
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_RandomForest")]] <- oblique_res
}

```

# PLOT

```{r}

# Access results using results_list[['Recipe_1_Weibull']], etc.
# Function to extract and prepare metrics for plotting
prepare_survival_data <- function(result, model_name, feature_set_name) {
  collect_metrics(result, summarize = FALSE) %>%
    mutate(Model = model_name,  feature_set = feature_set_name)
}


# Initialize an empty data frame to store all metrics
all_metrics <- data.frame()
View(all_metrics)

# Save the final table with median, quartiles, and tuning parameters as a CSV
# write.csv(all_metrics, "survivalmodelresults.csv", row.names = FALSE)

# Loop through results and extract metrics for each model and recipe
for (name in names(results_list)) {
  model_name <- str_extract(name, "Weibull|Cox|RandomForest")
  recipe_name <- str_extract(name, "(?<=Feature_Set_)[^_]+")  
  metrics_data <- prepare_survival_data(results_list[[name]], model_name, recipe_name)
  all_metrics <- bind_rows(all_metrics, metrics_data)
}

# Summarize metrics for plotting
summary_survival_metrics <- all_metrics %>%
  group_by(feature_set, Model, .metric)
summary_survival_metrics

# Plot Brier score and ROC AUC for all models and recipes
SurvivalAllModels_0 <- ggplot(summary_survival_metrics, aes(x = feature_set, y = .estimate, fill = Model)) +
  geom_boxplot() +
  facet_wrap(~.metric, scales = "free_y") +
  labs(title = "Survival Analysis - Demographics Across All Models", x = "Feature Set(s)", y = "Metric Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 13) + 
  theme(legend.position = 'bottom')

# Print the box plot
print(SurvivalAllModels_0)


# Save the updated plot
ggsave("SurvivalAllModels_0.png", plot = SurvivalAllModels_0, width = 10, height = 10)
```

# Ordered plotting (worst performing model should be on the left and best on the right)

```{r}


# Step 1: Calculate mean performance for ordering
model_order <- all_metrics %>%
  group_by(feature_set, Model, .metric) %>%
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE), .groups = "drop")

View(model_order)
# Order feature sets by average ROC AUC (or your chosen metric)
best_feature_order <- model_order %>%
  filter(.metric == "roc_auc_survival") %>%      # Filter to use "roc_auc" or another metric for ordering
  arrange(mean_estimate) %>%            # Sort in ascending order for worst to best
  distinct(feature_set) %>%             # Remove duplicate levels if any
  pull(feature_set)

best_feature_order
# Ensure the factor levels are unique in the desired order
all_metrics$feature_set <- factor(all_metrics$feature_set, levels = unique(best_feature_order))


# Plot Brier score and ROC AUC for all models and recipes
SurvivalAllModels_ordered <- ggplot(all_metrics, aes(x = feature_set, 
                                                     y = .estimate, 
                                                     fill = Model)) +
  geom_boxplot() +
  facet_wrap(~.metric, scales = "free_y") +
  labs(title = "Survival Analysis - Demographics Across All Models", 
       x = "Feature Set(s)", 
       y = "Metric Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 13) + 
  theme(legend.position = 'bottom')

# Print the ordered box plot
print(SurvivalAllModels_ordered)

# Save the ordered plot
ggsave("SurvivalAllModels_ordered.png", plot = SurvivalAllModels_ordered, width = 10, height = 10)
```

# Print & save only the AUC ROC

```{r}

# Filter the data to include only the ROC AUC metric
all_metrics_auc <- all_metrics %>%
  filter(.metric == "roc_auc_survival")

# Plot only ROC AUC for all models and recipes
SurvivalAUC_Only <- ggplot(all_metrics_auc, aes(x = feature_set, y = .estimate, fill = Model)) +
  geom_boxplot() +
  labs(title = "ROC AUC Across All Models and Feature Sets", x = "Feature Set(s)", y = "AUC Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 13) + 
  theme(legend.position = 'bottom')

# Print the AUC ROC plot
print(SurvivalAUC_Only)

# Save the AUC ROC plot
ggsave("SurvivalAUC_Only.png", plot = SurvivalAUC_Only, width = 10, height = 10)

```

# TABLE

```{r}

# Function to extract the best parameters for survival models
# For Cox PH  (coxnet_res)
cox_best_params <- coxnet_res %>%
  select_best(metric = "roc_auc_survival") %>%
  mutate(Model = "Cox")

# For RF_survival (oblique_res)
oblique_best_params <- oblique_res %>%
  select_best(metric = "roc_auc_survival") %>%
  mutate(Model = "RandomForest")


# Combine all the best tuning parameters
best_params <- bind_rows(cox_best_params, oblique_best_params)

# Calculate summary statistics for ROC AUC values
summary_metrics1 <- all_metrics %>%
  group_by(feature_set, Model, .metric) %>%
  summarize(
    Median = median(.estimate, na.rm = TRUE),
    Q1 = quantile(.estimate, 0.25, na.rm = TRUE),
    Q3 = quantile(.estimate, 0.75, na.rm = TRUE),
  )

# Join the summary statistics with the best tuning parameters
final_summary1 <- summary_metrics1 %>%
  left_join(best_params, by = "Model")

# Filter for rows where .metric == "roc_auc" and round numeric columns
roc_auc_surv_summary <- final_summary1 %>%
  filter(.metric == "roc_auc_survival") %>%
  mutate(across(c(Median, Q1, Q3), round, 3))  # Round Median, Q1, Q3 to 3 decimal places

# Print the filtered summary table
print(roc_auc_surv_summary)

# Save the final table with median, quartiles, and tuning parameters as a CSV
# write.csv(roc_auc_summary, "roc_auc_summary_with_tuning_params.csv", row.names = FALSE)

```

# External Validation on UKBB & AIBL

```{r}

# Import preprocessed data 
ADNI_subset <- read.csv("ADNI_subset.csv")

# View(ADNI_subset)

AIBL_subset <- read.csv("AIBL_subset.csv")
View(AIBL_subset)

UKBB_subset <- read.csv("UKBB_subset.csv")
# View(UKBB_subset)

# View(surv_df)
# Convert the event column to a factor
# UKBB_subset$conv <- factor(UKBB_subset$conv, levels = c(0, 1), labels = c("No AD", "AD"))
# 
# # Convert the event column to a factor
# AIBL_subset$conv <- factor(AIBL_subset$conv, levels = c(0, 1), labels = c("No AD", "AD"))

# Import preprocessed data 
surv_df <- read.csv("preprocessed_data.csv") 
# View(surv_df)
# Convert PTGENDER to numeric (0 for Male, 1 for Female)
surv_df$SEX <- ifelse(surv_df$SEX == "Male", 0, ifelse(surv_df$SEX == "Female", 1, NA))

# Check if any NAs were introduced (for troubleshooting)
sum(is.na(surv_df$SEX))
# Create survival object
surv_df1 <- surv_df %>% 
  mutate(diagnosis_surv = Surv(VISCODE, event)) %>%
  select(diagnosis_surv, everything())

View(surv_df1)

# Create survival object
AIBL_subset <- AIBL_subset %>% 
  mutate(diagnosis_surv = Surv(event_time, conv)) %>%
  select(diagnosis_surv, everything())

# Create survival object
UKBB_subset <- UKBB_subset %>% 
  mutate(diagnosis_surv = Surv(event_time, conv)) %>%
  select(diagnosis_surv, everything())

# Convert SEX column to numeric in all datasets to prevent coercion issues
ADNI_subset$SEX <- as.numeric(ADNI_subset$SEX)
AIBL_subset$SEX <- as.numeric(AIBL_subset$SEX)
UKBB_subset$SEX <- as.numeric(UKBB_subset$SEX)

# Convert `conv` to binary format (0/1)
# UKBB_subset$conv <- ifelse(UKBB_subset$conv == "No AD", 0, 1)
# AIBL_subset$conv <- ifelse(AIBL_subset$conv == "No AD", 0, 1)

```

```{r}

library(tidymodels)
library(dplyr)
library(broom)

# Define recipes for models
recipes_list <- list(
  D = recipe(diagnosis_surv ~ AGE + SEX + EDU, data = surv_df1),
  Fh = recipe(diagnosis_surv ~ FH, data = surv_df1),
  DFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + FH, data = surv_df1),
  DGFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + APOE4 + FH, data = surv_df1)
)


#########################################################
# Iterate through recipes and run models for each
for (i in seq_along(recipes_list)) {
  # Select the current recipe
  current_recipe <- recipes_list[[i]]
  

  # Random Forest Workflow
  oblique_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(rand_forest(trees = 1000, 
                          mtry = mtry = 2, 
                          min_n = 1) %>%
                set_engine("aorsf") %>%
                set_mode("censored regression"))



  # Define evaluation metrics for survival analysis
  survival_metrics <- metric_set(brier_survival_integrated, 
                                 roc_auc_survival, 
                                 concordance_survival)
  
  # Set evaluation time points for metrics
  evaluation_time_points <- seq(0, 144, 6)

  # Cross-validation resampling (for all models)
  set.seed(1)
  oblique_res <- tune_grid(oblique_wflow, 
                           resamples = repeated_folds, 
                           grid = rf_grid, 
                           metrics = survival_metrics, 
                           eval_time = evaluation_time_points, 
                           control = control_grid(save_pred = TRUE))
  
  # Store the results for each model
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_RandomForest")]] <- oblique_res
}


  # Get predictions and calculate ROC AUC for UKBB
  preds_ukbb <- predict(fit, 
                        new_data = UKBB_subset, 
                        type = "survival", 
                        eval_time = evaluation_time_points)
  print(head(preds_ukbb)) 
  
  roc_auc_ukbb <- survival_metrics(preds_ukbb, 
                                   truth = UKBB_subset$conv, 
                                   .eval_time = evaluation_time_points)
  
  # Get predictions and calculate ROC AUC for AIBL
  preds_aibl <- predict(fit, 
                        new_data = AIBL_subset, 
                        type = "survival", 
                        eval_time = evaluation_time_points)
  
  roc_auc_aibl <- survival_metrics(preds_aibl, 
                                   truth = AIBL_subset$conv, 
                                   .eval_time = evaluation_time_points)

  # Store the results
  roc_auc_results <- roc_auc_results %>%
    add_row(
      Model = model_name,
      Train_ROC_AUC = round(mean(roc_auc_train$.estimate), 3),
      UKBB_ROC_AUC = round(mean(roc_auc_ukbb$.estimate), 3),
      AIBL_ROC_AUC = round(mean(roc_auc_aibl$.estimate), 3)
    )
}

# View results
print(roc_auc_results)

```

```{r}
library(tidymodels)
library(dplyr)
library(broom)

# Define recipes for models
recipes_list <- list(
  D = recipe(diagnosis_surv ~ AGE + SEX + EDU, data = surv_df1),
  Fh = recipe(diagnosis_surv ~ FH, data = surv_df1),
  DFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + FH, data = surv_df1),
  DGFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + APOE4 + FH, data = surv_df1)
)

# List to store results
roc_auc_results <- tibble(Model = character(), Train_ROC_AUC = numeric(), UKBB_ROC_AUC = numeric(), AIBL_ROC_AUC = numeric())

# Define evaluation metrics for survival analysis
survival_metrics <- metric_set(roc_auc_survival, concordance_survival)
evaluation_time_points <- seq(0, 144, 6)

# Iterate through recipes and run models for each
for (i in seq_along(recipes_list)) {
  # Select the current recipe
  current_recipe <- recipes_list[[i]]
  
  # Random Forest Workflow without tuning
  oblique_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(
      rand_forest(trees = 1000, mtry = 2, min_n = 1) %>%
        set_engine("aorsf") %>%
        set_mode("censored regression")
    )

  # Fit the model to the training data
  final_fit <- fit(oblique_wflow, data = surv_df1)

  # Get predictions for the training set
  preds_train <- predict(final_fit, new_data = surv_df1, type = "survival", eval_time = evaluation_time_points)

  # Extract the survival probabilities at the evaluation time points
  train_probs <- preds_train %>% 
    select(-.pred) %>%  # Remove the first column which is usually the time column
    as.data.frame()  # Convert to a data frame for easier manipulation

  # Calculate ROC AUC for the training set
  roc_auc_train <- roc_auc_survival(train_probs, truth = surv_df1$event, .eval_time = evaluation_time_points)

  # Get predictions and calculate ROC AUC for UKBB
  preds_ukbb <- predict(final_fit, new_data = UKBB_subset, type = "survival", eval_time = evaluation_time_points)
  ukbb_probs <- preds_ukbb %>% select(-.pred) %>% as.data.frame()
  roc_auc_ukbb <- roc_auc_survival(ukbb_probs, truth = UKBB_subset$conv, .eval_time = evaluation_time_points)

  # Get predictions and calculate ROC AUC for AIBL
  preds_aibl <- predict(final_fit, new_data = AIBL_subset, type = "survival", eval_time = evaluation_time_points)
  aibl_probs <- preds_aibl %>% select(-.pred) %>% as.data.frame()
  roc_auc_aibl <- roc_auc_survival(aibl_probs, truth = AIBL_subset$conv, .eval_time = evaluation_time_points)

  # Calculate mean ROC AUC for each dataset and store results
  model_name <- paste0("Feature_Set_", names(recipes_list)[i], "_RandomForest")
  roc_auc_results <- roc_auc_results %>%
    add_row(
      Model = model_name,
      Train_ROC_AUC = round(mean(roc_auc_train$.estimate), 3),
      UKBB_ROC_AUC = round(mean(roc_auc_ukbb$.estimate), 3),
      AIBL_ROC_AUC = round(mean(roc_auc_aibl$.estimate), 3)
    )
}

# View results
print(roc_auc_results)

range(surv_df1$VISCODE)
range(UKBB_subset$event_time)
range(AIBL_subset$event_time)

```

```{r}

library(tidymodels)
library(dplyr)
library(broom)

# Copy the original datasets to add predictions to
updated_UKBB_subset_SA <- UKBB_subset
updated_AIBL_subset_SA <- AIBL_subset

# Loop over each model and get predictions
for (model_name in names(fit_models)) {
  fit <- fit_models[[model_name]]
  
  # Generate predictions (class labels) for each subset and rename columns
  preds_ukbb_SA <- predict(fit, new_data = UKBB_subset) %>% pull(.pred_class)
  preds_aibl_SA <- predict(fit, new_data = AIBL_subset) %>% pull(.pred_class)
  
  # Add predictions as new columns in each updated dataset
  updated_UKBB_subset_SA[[model_name]] <- preds_ukbb_SA
  updated_AIBL_subset_SA[[model_name]] <- preds_aibl_SA
}

# View updated datasets with predictions
View(updated_UKBB_subset_SA)
View(updated_AIBL_subset_SA)


```

-   **Brier Survival** (Top left): This metric evaluates the accuracy of survival probability predictions. Lower values indicate better model performance. In this plot, we see the Cox model (red) has more variability in some recipes, while the RandomForest and Weibull models (green, blue) show less variation and seem to perform more consistently across recipes.

-   **Brier Survival Integrated** (Top right): This integrated Brier score is similar to the Brier survival score but integrates over time. Again, a lower value indicates better performance. In Recipe 1, the Cox model performs worse than the other two models, but the performance gap reduces in Recipes 2 and 3.

-   **Concordance Survival** (Bottom left): Concordance is used to evaluate the discriminative ability of the models. It ranges from 0.5 (random guessing) to 1 (perfect prediction). The Cox model shows the most variability in Recipe 1 but stabilizes in the other recipes. RandomForest and Weibull models have relatively stable concordance across recipes.

-   **ROC- AUC Survival** (Bottom right): This metric measures the area under the receiver operating characteristic curve. Higher values (closer to 1) indicate better model performance. The RandomForest and Weibull models perform similarly, while the Cox model has higher variability and performs worse, especially in Recipe 1
