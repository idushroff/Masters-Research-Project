---
title: "ADNI Data V2 - Survival Analysis Predictive Models"
output: html_document
date: "2023-03-29"
---

# ADNI Data V2 - Survival Analysis Predictive Models 

# Same as Version 1 except this time I implemented K-fold cross validation on the training data 

## Install and load the relevant packages

```{r}

# install.packages("caret")
library(caret)

# install.packages("ranger")
library(ranger)

# install.packages("tidymodels")
library(tidymodels)

# install.packages("tidyverse")
library(tidyverse)

# install.packages("glmnet")
library(glmnet)

# install.packages("modeldatatoo")
library(modeldatatoo)

# install.packages("aorsf")
library(aorsf)

# install.packages("censored")
library(censored)

# install.packages("survival")
library(survival)

# install.packages("doParallel")
library(doParallel)

library(pROC) #for AUC calculation

# Install and load necessary packages
# install.packages("tidymodels")
library(tidymodels)

# install.packages("survival")
library(survival)

# install.packages("doParallel")
library(doParallel)

# Set up parallel processing
registerDoParallel()
```

# Weibull distribution - censored regression

```{r}

 # Import preprocessed data 
surv_df <- read.csv("preprocessed_data.csv") 
# View(surv_df)

# In the dataset, we can see the time to event in the VISCODE column as well as the status of the participant. When the time to event for a participant with the status "No AD" is labeled as censored, it means that the event of interest (such as progression to Alzheimer's disease) did not occur within the observed time period. This could be due to various reasons; the study ending before the event could happen to that participant, or simply because the event has not yet occurred at the time of data collection. 

# Remove rows with zero survival times
surv_df <- surv_df %>% filter(VISCODE > 0)

# Check event encoding
# table(surv_df$event)

#   0   1 
# 954 380 

surv_df1 <- surv_df %>% 
  mutate(diagnosis_surv = Surv(VISCODE, event == "1")) %>%
  select(diagnosis_surv, everything())
# View(surv_df1)

# For our resampling strategy, letâ€™s use a 3-way split into training, validation, and test set.
set.seed(403)

library(caTools)

split <- sample.split(surv_df1$event, SplitRatio=0.75)
train_data <- subset(surv_df1, split == TRUE) 
test_data <- subset(surv_df1, split == FALSE)

# Recipe for preprocessing the data
surv_recipe <- recipe(diagnosis_surv ~ AGE + APOE4, data = train_data)

# Define the model: Weibull distribution survival regression
survreg_spec <- survival_reg() %>% 
  set_engine("survival") %>% 
  set_mode("censored regression")

# Create workflow combining the recipe and the model
survreg_wflow <- workflow() %>% 
  add_recipe(surv_recipe) %>% 
  add_model(survreg_spec)

# Set up repeated k-fold cross-validation
set.seed(123)
folds <- vfold_cv(train_data, v = 5, repeats = 3)

# Define evaluation metrics for survival analysis
survival_metrics <- metric_set(brier_survival_integrated, brier_survival, roc_auc_survival, concordance_survival)

# Set evaluation time points for metrics
evaluation_time_points <- seq(0, 144, 6)

# Fit the model using repeated k-fold cross-validation
survreg_res <- fit_resamples(
  survreg_wflow,
  resamples = folds,
  metrics = survival_metrics,
  eval_time = evaluation_time_points,
  control = control_resamples(save_pred = TRUE)
)

# Collect and review predictions
preds <- collect_predictions(survreg_res)
preds$.pred[[6]]

# Plot the ROC AUC over time
collect_metrics(survreg_res) %>%
  filter(.metric == "roc_auc_survival") %>%
  ggplot(aes(.eval_time, mean)) +
  geom_line() +
  labs(x = "Evaluation Time (VISCODE)", y = "Area Under the ROC Curve") +
  ggtitle("Weibull Distribution - AUC ROC Curve Over Time")

# Plot the Brier score over time
collect_metrics(survreg_res) %>%
  filter(.metric == "brier_survival") %>%
  ggplot(aes(.eval_time, mean)) +
  geom_line() +
  labs(x = "Evaluation Time", y = "Brier Score") +
  ggtitle("Weibull Distribution - Brier Score Over Time")

# Compare to other models (e.g., Random Forest and Penalized Cox)

# Define an oblique random forest model
oblique_spec <- rand_forest(mtry = tune(), min_n = tune()) %>% 
  set_engine("aorsf") %>% 
  set_mode("censored regression")

# Workflow for oblique random forest model
oblique_wflow <- workflow() %>% 
  add_recipe(surv_recipe) %>% 
  add_model(oblique_spec)

# Define a penalized Cox Proportional Hazards model
coxnet_spec <- proportional_hazards(penalty = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("censored regression")

# Workflow for Cox model
coxnet_wflow <- workflow() %>% 
  add_recipe(surv_recipe) %>% 
  add_model(coxnet_spec)

# Tuning and resampling for the oblique random forest model using repeated k-fold cross-validation
set.seed(1)
oblique_res <- tune_grid(
  oblique_wflow,
  resamples = folds,
  grid = 10,
  metrics = survival_metrics,
  eval_time = evaluation_time_points,
  control = control_grid(save_workflow = TRUE)
)

# Tuning and resampling for the penalized Cox model using repeated k-fold cross-validation
set.seed(1)
coxnet_res <- tune_grid(
  coxnet_wflow,
  resamples = folds,
  grid = 10,
  metrics = survival_metrics,
  eval_time = evaluation_time_points,
  control = control_grid(save_workflow = TRUE)
)

# Select the best performing models
show_best(oblique_res, metric = "brier_survival_integrated", n = 5)
show_best(coxnet_res, metric = "brier_survival_integrated", n = 5)

# Finalizing the best Random Forest model
param_best <- select_best(oblique_res, metric = "brier_survival_integrated")
last_oblique_wflow <- finalize_workflow(oblique_wflow, param_best)

# Fit the final model on the entire dataset and evaluate it
set.seed(2)
last_oblique_fit <- last_fit(
  last_oblique_wflow, 
  split = data_split,
  metrics = survival_metrics,
  eval_time = evaluation_time_points
)

# Collect final metrics from the last fitted model
collect_metrics(last_oblique_fit) %>% 
  filter(.metric == "brier_survival_integrated")

# Plot comparison of Brier score between validation and test sets
brier_val <- collect_metrics(oblique_res) %>% 
  filter(.metric == "brier_survival") %>% 
  filter(mtry == param_best$mtry, min_n == param_best$min_n) %>% 
  mutate(Data = "Validation")

brier_test <- collect_metrics(last_oblique_fit) %>% 
  filter(.metric == "brier_survival") %>% 
  mutate(Data = "Testing") %>% 
  rename(mean = .estimate)

bind_rows(brier_val, brier_test) %>% 
  ggplot(aes(.eval_time, mean, col = Data)) + 
  geom_line() + 
  labs(x = "Evaluation Time", y = "Brier Score")
```

\# Interpretation of Brier Score Plot:

The Brier score is a measure of the accuracy of the predicted probabilities.

At the beginning of the evaluation period (near time 0), the Brier score is low, indicating high accuracy in the short-term predictions. This is expected because there are likely few events (deaths or disease progressions) early on, so the survival probabilities are close to 1, leading to low Brier scores.

As time progresses, the Brier score increases. This suggests that the accuracy of the survival predictions decreases over time. This could be due to a number of factors such as more events occurring, making predictions harder, or the model not capturing long-term survival trends well.

The score fluctuates over time, which indicates variability in the model's accuracy at different time points. This can occur if the number of events changes significantly at different times or if the model's assumptions do not hold uniformly across all time points.

Towards the end of the evaluation period, the Brier score spikes, indicating poor predictive accuracy. This could be due to a small number of subjects remaining in the study, leading to higher uncertainty and less reliable predictions.
