---
title: "ADNI Data V2 - Survival Analysis Predictive Models"
output: html_document
date: "2023-03-29"
---

# 4_Predictive_Analysis_Survival_A\_Models

## Install and load the relevant packages

```{r}

# install.packages("caret")
library(caret)

# install.packages("ranger")
library(ranger)

# install.packages("tidymodels")
library(tidymodels)

# install.packages("tidyverse")
library(tidyverse)

# install.packages("glmnet")
library(glmnet)

# install.packages("modeldatatoo")
library(modeldatatoo)

# install.packages("aorsf")
library(aorsf)

# install.packages("censored")
library(censored)

# install.packages("survival")
library(survival)

# install.packages("doParallel")
library(doParallel)

library(pROC) #for AUC calculation

# install.packages("survival")
library(survival)

# install.packages("doParallel")
library(doParallel)

# Set up parallel processing
registerDoParallel()

# Load the randomForestSRC package
# install.packages("randomForestSRC")
# library(randomForestSRC)

library(dplyr)
library(broom)
```

# Import data set, Pre-process & set up cross validation

```{r}

# Import preprocessed data 
surv_df <- read.csv("preprocessed_data.csv") 
# View(surv_df)

# Create survival object
surv_df1 <- surv_df %>% 
  mutate(diagnosis_surv = Surv(VISCODE, event)) %>%
  select(diagnosis_surv, everything())
# View(surv_df1)

# Convert PTGENDER to numeric (0 for Male, 1 for Female)
surv_df1 <- surv_df %>%
  mutate(SEX = ifelse(SEX == "Male", 0, 1))

# Define resampling scheme: repeated k-fold cross-validation
set.seed(403)
repeated_folds <- vfold_cv(surv_df1, v = 5, repeats = 3) # 5-fold cross-validation

# List of different recipes based on specified feature combinations
recipes_list <- list(
  D = recipe(diagnosis_surv ~ AGE + SEX + EDU, data = surv_df1),
  Fh = recipe(diagnosis_surv ~ FH, data = surv_df1),
  DFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + FH, data = surv_df1),
  DGFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + APOE4 + FH, data = surv_df1),
  DGCFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + APOE4 + MMSE + mPACCtrailsB + FH, data = surv_df1),
  DGC = recipe(diagnosis_surv ~ APOE4 + AGE + SEX + EDU + MMSE + mPACCtrailsB, data = surv_df1)
)

# Initialize list to store results for each recipe and model
results_list <- list()

# Define tuning grid for Cox model (regularization penalty)
cox_grid <- expand.grid(
  penalty = c(0.001 ,0.01, 0.1, 1, 10, 100) # Specific values for regularization penalty
)


# Define tuning grid for Random Forest
rf_grid <- expand.grid(
  mtry = c(2, 3, 4),  # Number of variables randomly sampled at each split
  min_n = c(1, 2, 4)  # Minimum number of samples in a terminal node
)
```

# TRAIN

```{r}

# Iterate through recipes and run models for each
for (i in seq_along(recipes_list)) {
  # Select the current recipe
  current_recipe <- recipes_list[[i]]
  
  # Define models: Weibull, Cox, Random Forest
  # Weibull Model
  survreg_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(survival_reg() %>%
                set_engine("survival") %>%
                set_mode("censored regression"))

  # Cox Proportional Hazards Model
  predictors=current_recipe$var_info %>% filter(role=="predictor") %>% pull(variable) %>% paste(collapse="+")
  coxnet_wflow <- workflow() %>%
     add_formula(as.formula(str_c("Surv(VISCODE, event) ~ ", predictors))) %>%
     add_model(proportional_hazards(penalty = tune(), mixture=0) %>% 
                 set_engine("glmnet") %>% 
                 set_mode("censored regression"))
  
  # Random Forest Workflow
  oblique_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(rand_forest(trees = 1000, 
                          mtry = tune(), 
                          min_n = tune()) %>%
                set_engine("aorsf") %>%
                set_mode("censored regression"))



  # Define evaluation metrics for survival analysis
  survival_metrics <- metric_set(brier_survival_integrated, 
                                 roc_auc_survival, 
                                 concordance_survival)
  
  # Set evaluation time points for metrics
  evaluation_time_points <- seq(0, 144, 6)

  # Cross-validation resampling (for all models)
  set.seed(1)
  survreg_res <- fit_resamples(survreg_wflow, 
                               resamples = repeated_folds, 
                               metrics = survival_metrics, 
                               eval_time = evaluation_time_points, 
                               control = control_resamples(save_pred = TRUE))
  

  coxnet_res <- tune_grid(coxnet_wflow, 
                          resamples = repeated_folds, 
                          grid = cox_grid, 
                          metrics = survival_metrics, 
                          eval_time = evaluation_time_points, 
                          control = control_grid(save_pred = TRUE))
  
  oblique_res <- tune_grid(oblique_wflow, 
                           resamples = repeated_folds, 
                           grid = rf_grid, 
                           metrics = survival_metrics, 
                           eval_time = evaluation_time_points, 
                           control = control_grid(save_pred = TRUE))
  
  # Store the results for each model
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_Weibull")]] <- survreg_res
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_Cox")]] <- coxnet_res
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_RandomForest")]] <- oblique_res
}

```

# Ordered plotting (worst performing model should be on the left and best on the right)

```{r}

# # Import preprocessed data 
all_metrics <- read.csv("survivalmodelresults.csv")
all_metrics
# Step 1: Calculate mean performance for ordering
model_order <- all_metrics %>%
  group_by(feature_set, Model, .metric) %>%
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE), .groups = "drop")

View(model_order)
# Order feature sets by average ROC AUC (or your chosen metric)
best_feature_order <- model_order %>%
  filter(.metric == "roc_auc_survival") %>%      # Filter to use "roc_auc" or another metric for ordering
  arrange(mean_estimate) %>%            # Sort in ascending order for worst to best
  distinct(feature_set) %>%             # Remove duplicate levels if any
  pull(feature_set)

best_feature_order
# Ensure the factor levels are unique in the desired order
all_metrics$feature_set <- factor(all_metrics$feature_set, levels = unique(best_feature_order))


# Plot Brier score and ROC AUC for all models and recipes
SurvivalAllModels_ordered <- ggplot(all_metrics, aes(x = feature_set, 
                                                     y = .estimate, 
                                                     fill = Model)) +
  geom_boxplot() +
  facet_wrap(~.metric, scales = "free_y") +
  labs(title = "Survival Analysis - Demographics Across All Models", 
       x = "Feature Set(s)", 
       y = "Metric Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 13) + 
  theme(legend.position = 'bottom')

# Print the ordered box plot
print(SurvivalAllModels_ordered)

# Save the ordered plot
ggsave("SurvivalAllModels_ordered.png", plot = SurvivalAllModels_ordered, width = 10, height = 10)
```

-   **Brier Survival** (Top left): This metric evaluates the accuracy of survival probability predictions. Lower values indicate better model performance. In this plot, we see the Cox model (red) has more variability in some recipes, while the RandomForest and Weibull models (green, blue) show less variation and seem to perform more consistently across recipes.

-   **Brier Survival Integrated** (Top right): This integrated Brier score is similar to the Brier survival score but integrates over time. Again, a lower value indicates better performance. In Recipe 1, the Cox model performs worse than the other two models, but the performance gap reduces in Recipes 2 and 3.

-   **Concordance Survival** (Bottom left): Concordance is used to evaluate the discriminative ability of the models. It ranges from 0.5 (random guessing) to 1 (perfect prediction). The Cox model shows the most variability in Recipe 1 but stabilizes in the other recipes. RandomForest and Weibull models have relatively stable concordance across recipes.

-   **ROC- AUC Survival** (Bottom right): This metric measures the area under the receiver operating characteristic curve. Higher values (closer to 1) indicate better model performance. The RandomForest and Weibull models perform similarly, while the Cox model has higher variability and performs worse, especially in Recipe 1

# Print & save only the AUC ROC

```{r}

# Filter the data to include only the ROC AUC metric
all_metrics_auc <- all_metrics %>%
  filter(.metric == "roc_auc_survival")

# Plot only ROC AUC for all models and recipes
SurvivalAUC_Only <- ggplot(all_metrics_auc, aes(x = feature_set, y = .estimate, fill = Model)) +
  geom_boxplot() +
  labs(title = "ROC AUC Across All Models and Feature Sets", x = "Feature Set(s)", y = "AUC Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 13) + 
  theme(legend.position = 'bottom')

# Print the AUC ROC plot
print(SurvivalAUC_Only)

# Save the AUC ROC plot
ggsave("SurvivalAUC_Only.png", plot = SurvivalAUC_Only, width = 10, height = 10)

```

# Plot should be concordance index so here is:

```{r}

# Filter the data to include only the concordance index metric
all_metrics_concordance <- all_metrics %>%
  filter(.metric == "concordance_survival")

# Plot concordance index for all models and feature sets
ConcordanceIndex_Plot2 <- ggplot(all_metrics_concordance, aes(x = feature_set, y = .estimate, fill = Model)) +
  geom_boxplot() +
  labs(title = "Survival Analysis  - Comparison Across Feature Sets", x = "Feature Set(s)", y = "Concordance Index Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 25) + 
  theme(legend.position = 'bottom')

# Print the Concordance Index plot
print(ConcordanceIndex_Plot2)

# Save the Concordance Index plot
ggsave("ConcordanceIndex_Plot2.png", plot = ConcordanceIndex_Plot2, width = 10, height = 10)

```

# TABLE

```{r}

# Function to extract the best parameters for survival models
# For Cox PH  (coxnet_res)
cox_best_params <- coxnet_res %>%
  select_best(metric = "roc_auc_survival") %>%
  mutate(Model = "Cox")

# For RF_survival (oblique_res)
oblique_best_params <- oblique_res %>%
  select_best(metric = "roc_auc_survival") %>%
  mutate(Model = "RandomForest")


# Combine all the best tuning parameters
best_params <- bind_rows(cox_best_params, oblique_best_params)

# Calculate summary statistics for ROC AUC values
summary_metrics1 <- all_metrics %>%
  group_by(feature_set, Model, .metric) %>%
  summarize(
    Median = median(.estimate, na.rm = TRUE),
    Q1 = quantile(.estimate, 0.25, na.rm = TRUE),
    Q3 = quantile(.estimate, 0.75, na.rm = TRUE),
  )

# Join the summary statistics with the best tuning parameters
final_summary1 <- summary_metrics1 %>%
  left_join(best_params, by = "Model")

# Filter for rows where .metric == "roc_auc" and round numeric columns
roc_auc_surv_summary <- final_summary1 %>%
  filter(.metric == "roc_auc_survival") %>%
  mutate(across(c(Median, Q1, Q3), round, 3))  # Round Median, Q1, Q3 to 3 decimal places

# Print the filtered summary table
print(roc_auc_surv_summary)

# Save the final table with median, quartiles, and tuning parameters as a CSV
# write.csv(roc_auc_summary, "roc_auc_summary_with_tuning_params.csv", row.names = FALSE)

```

# External Validation on UKBB & AIBL

```{r}

# Import pre-processed datasets
AIBL_subset <- read.csv("AIBL_subset.csv")
# View(AIBL_subset)
UKBB_subset <- read.csv("UKBB_subset.csv")
# View(UKBB_subset)
surv_df <- read.csv("preprocessed_data.csv") 
# View(surv_df)

# Convert PTGENDER to numeric (0 for Male, 1 for Female)
surv_df$SEX <- ifelse(surv_df$SEX == "Male", 0, ifelse(surv_df$SEX == "Female", 1, NA))
AIBL_subset$SEX <- as.numeric(AIBL_subset$SEX)
UKBB_subset$SEX <- as.numeric(UKBB_subset$SEX)

# Create survival object
surv_df1 <- surv_df %>% 
  mutate(diagnosis_surv = Surv(VISCODE, event)) %>%
  select(diagnosis_surv, everything())

# Create survival object
AIBL_subset <- AIBL_subset %>% 
  mutate(diagnosis_surv = Surv(event_time, conv)) %>%
  select(diagnosis_surv, everything())

# Create survival object
UKBB_subset <- UKBB_subset %>% 
  mutate(diagnosis_surv = Surv(event_time, conv)) %>%
  select(diagnosis_surv, everything())

#######################################################################
# Define recipes for models
recipes_list <- list(
  D = recipe(diagnosis_surv ~ AGE + SEX + EDU, data = surv_df1),
  Fh = recipe(diagnosis_surv ~ FH, data = surv_df1),
  DFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + FH, data = surv_df1),
  DGFh = recipe(diagnosis_surv ~ AGE + SEX + EDU + APOE4 + FH, data = surv_df1)
)



oblique_spec <- rand_forest(trees = 1000, mtry = 2, min_n = 1) %>% 
  set_engine("aorsf") %>% 
  set_mode("censored regression")

# Define time points for AUC evaluation
evaluation_time_points <- seq(0, 144, 6)

# List workflows for each model
models_list <- list(
  D_RSF = workflow() %>% 
    add_recipe(recipes_list[["D"]]) %>% 
    add_model(oblique_spec),
  
  Fh_RSF = workflow() %>% 
    add_recipe(recipes_list[["Fh"]]) %>% 
    add_model(oblique_spec),
  
  DFh_RSF = workflow() %>% 
    add_recipe(recipes_list[["DFh"]]) %>% 
    add_model(oblique_spec),
  
  DGFh_RSF = workflow() %>% 
    add_recipe(recipes_list[["DGFh"]]) %>% 
    add_model(oblique_spec)
)


# Cross-validation resampling setup
set.seed(1)
repeated_folds <- vfold_cv(surv_df1, v = 5, repeats = 3)
dynamic_survival_metrics <- metric_set(brier_survival_integrated, 
                                 roc_auc_survival)
  
# Train and evaluate models on multiple datasets
res=lapply(names(fit_models), \(model_name) {
  print(str_c('Evaluating ', model_name))
  fit <- fit_models[[model_name]]
  
  # Get predictions and calculate ROC AUC for ADNI (Training Set)
  preds_train <- predict(fit, 
                         new_data = surv_df1, 
                         type = "survival", 
                         eval_time = evaluation_time_points)
  print(preds_train)
  # https://www.tidymodels.org/learn/statistics/survival-metrics/
  # I could probabl use augment()) for this
  UKBB_subset$event_surv = Surv(UKBB_subset$event_time, 1*(UKBB_subset$conv=="1") )
  preds_ukbb_time <- predict(fit, new_data = UKBB_subset, type = 'time')
  print(head(preds_ukbb_time))
  
  preds_ukbb_surv <- predict(fit, 
                        new_data = UKBB_subset,
                         eval_time = evaluation_time_points, type = 'survival')
  
  
  preds_ukbb_surv$.pred_time <- preds_ukbb_time$.pred_time

  preds_ukbb_surv$surv_obj <- UKBB_subset$event_surv
  preds_ukbb_surv = .censoring_weights_graf(fit, preds_ukbb_surv)
 

  surv_metrics_ukbb <- dynamic_survival_metrics(preds_ukbb_surv, 
                                     truth = surv_obj, 
                                      .pred)
  cindex_ukbb <- concordance_survival(preds_ukbb_surv, 
                                     truth = surv_obj, 
                                      estimate=.pred_time)
  res_ukb=bind_rows(surv_metrics_ukbb, cindex_ukbb)
  
  # Get predictions and calculate ROC AUC for AIBL
  AIBL_subset$event_surv = Surv(AIBL_subset$event_time, 1*(AIBL_subset$conv=="1") )

  preds_aibl_time <- predict(fit, 
                         new_data = AIBL_subset, 
                        type = 'time')
  preds_aibl_surv <- predict(fit, 
                        new_data = AIBL_subset,
                         eval_time = evaluation_time_points, type = 'survival')
  
  preds_aibl_surv$.pred_time = preds_aibl_time$.pred_time
  preds_aibl_surv$surv_obj = AIBL_subset$event_surv
  preds_aibl_surv = .censoring_weights_graf(fit, preds_aibl_surv)
 
  surv_metrics_aibl <- dynamic_survival_metrics(preds_aibl_surv, 
                                     truth = surv_obj, 
                                      .pred)
  cindex_aibl <- concordance_survival(preds_aibl_surv, 
                                     truth = surv_obj, 
                                      estimate=.pred_time)
  res_aibl=bind_rows(surv_metrics_aibl, cindex_aibl)
 
  res_aibl %>% mutate(study='AIBL') %>% bind_rows(res_ukb %>% mutate(study='UKBB')) %>% mutate(model=model_name)
})

# View results
res_all<-bind_rows(res)
print(res_all)
```

```{r}

concordance_results <- res_all %>% 
    filter(.metric == "concordance_survival")

# Display the extracted rows
print(concordance_results)
```

```{r}

brier_i_results <- res_all %>% 
    filter(.metric == "brier_survival_integrated")

# Display the extracted rows
print(brier_i_results)
```
