---
title: "ADNI Data V2 - Binary Classification  Predictive Models"
output: html_document
date: "2023-03-29"
---

# ADNI Data V2 - Binary Classification Predictive Models

## Install and load the relevant packages

```{r}

# Install and load the caret package, which provides a set of functions for training and plotting classification and regression models.
# install.packages("caret")
library(caret)


# Install and load the ranger package, which provides an efficient implementation of random forests.
# install.packages("ranger")
library(ranger)

# Install and load the tidymodels package, which provides a framework for modeling and machine learning.
# install.packages("tidymodels")
library(tidymodels)

# library(parsnip) # Install the parsnip package, for modeling. Useful because it provides a consistent interface to many different types of models.
# library(rsample) # For resampling methods; data splitting and resampling methods, such as cross-validation and bootstrapping.
# library(recipes) # For preprocessing and feature engineering tasks, such as data transformation and variable selection.
# library(workflows) # Provides a framework for creating and managing modeling workflows, allowing you to define and execute a series of preprocessing, modeling, and evaluation steps in a coherent manner.
# library(yardstick) # Install and load the yardstick package, which provides tools for evaluating models with tidy data principles.


# Install and load the tidyverse package, which is a collection of packages for data manipulation and visualization.
# install.packages("tidyverse")
library(tidyverse)

# readr: For reading CSV files
# dplyr: For data manipulation
# ggplot2:For plots and graphs
# tidyr: For tidying and reshaping data into a tidy format.
# purrr: For functional programming and working with lists and vectors.
# tibble: For creating and working with modern data frames.
# stringr: For string manipulation and text processing.
# forcats: For working with categorical data and factors.
# lubridate: For working with dates and times.
# magrittr: For creating expressive pipelines using the pipe operator %>%.
# rlang: For advanced manipulation and programming with R expressions.


# This package implements the elastic net regularization method for fitting generalized linear models (GLMs) and Cox proportional hazards models. Elastic net is a regularization technique that combines both L1 (lasso) and L2 (ridge) penalties to overcome some of the limitations of these individual methods, particularly in high-dimensional data settings. It's widely used for feature selection and regularization in regression and survival analysis.
# install.packages("glmnet")
library(glmnet)

# This package provides functions for creating synthetic datasets for modeling and simulation purposes. Synthetic datasets are artificially generated data that mimic the characteristics of real-world data. They are useful for testing models, evaluating algorithms, and conducting sensitivity analyses without relying on actual data, which may be limited or sensitive.
# install.packages("modeldatatoo")
library(modeldatatoo)

# This package stands for "Adaptive Orthogonal Random Survival Forests". It provides tools for building survival models using random forests with the adaptive orthogonal direction method. Random forests are an ensemble learning method for classification, regression, and survival analysis, and this package offers enhancements specifically for survival analysis.
# install.packages("aorsf")
library(aorsf)

# This package provides functions for analyzing survival data with censoring. Survival analysis deals with time-to-event data, where the "event" of interest could be death, failure, or any other predefined endpoint. Censoring occurs when the event of interest is not observed for some subjects within the study period. This package offers various methods and tools for handling censored survival data.
# install.packages("censored")
library(censored)


# Install packages for plotting survival analysis curves
# install.packages("survival")
library(survival)

# For parallel processing
# install.packages("doParallel")
library(doParallel)
```

## Building and Evaluating Predictive Models using the Tidymodels

# Import the preprocessed dataset and split into training and testing sets:

```{r}

# Import preprocessed data 
surv_df <- read.csv("preprocessed_data.csv") #survival_dataframe 

# Convert the event column to a factor
surv_df$event <- factor(surv_df$event, levels = c(0, 1), labels = c("No AD", "AD"))
# View(surv_df)

# Set seed for reproducibility
set.seed(123)

# Split the data into training and testing sets

library(caTools)

data_split <- sample.split(surv_df$event, SplitRatio=0.75)
train_data <- subset(surv_df, data_split == TRUE) 
test_data <- subset(surv_df, data_split == FALSE)

```

# Defining the different recipes

```{r}

# Define a recipe for preprocessing the data, using 'AGE' and 'APOE4' to predict 'event'
# surv_recipe <- recipe(event ~ AGE + APOE4 , data = train_data)

# Define a recipe for preprocessing the data using cognitive test scores and education level to predict 'event'
# surv_recipe <- recipe(event ~ mPACCtrailsB + MMSE + PTEDUCAT, data = train_data)

# Define a recipe for preprocessing the data using familial history predictors to predict 'event'
surv_recipe <- recipe(event ~ fam_hist_dad_dem + fam_hist_dad_ad + fam_hist_mum_dem + fam_hist_mum_ad, data = train_data)
```

# All Binary classification models - NO TUNING

```{r}

# Define a logistic regression model for classification
glm_model <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

# Define the Random forest model with hyperparameters set for tuning, settting engine to 'ranger' provides a fast implementation of random forests for this classification task & Set mode to 'classification' for binary outcomes
rf_model <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# Define a gradient boosting model for classification
gbm_model <- boost_tree() %>% 
  set_engine("xgboost", objective = "binary:logistic") %>% 
  set_mode("classification")

# Define 5-fold cross-validation with 3 repeats
cv_folds <- vfold_cv(train_data, v=5, repeats=3)

# Fit resamples
logistic_res <- fit_resamples(glm_model, surv_recipe, cv_folds)
randomForest_res <- fit_resamples(rf_model, surv_recipe, cv_folds)
XGBoost_res <- fit_resamples(gbm_model, surv_recipe, cv_folds)

# Create a tibble of model results
model_res <- tibble(model=list(logistic_res, randomForest_res, XGBoost_res),
       model_name = c("logistic", "randomforest", "XGBoost"))

# Create a helper function for collecting the metrics
map_collect_metrics <- function(model){model %>% 
    select(id, .metrics) %>% 
    unnest(.metrics)}

# Apply helper function and extract the metrics
model_res <- model_res %>% 
  mutate(res=map(model, map_collect_metrics)) %>% 
  select(model_name, res) %>% 
  unnest(res)

model_res #Display the model results

# Plot boxplots for each metric
plot <- model_res %>% 
  ggplot(aes(x=model_name, y= .estimate)) +
  geom_boxplot() + 
  facet_wrap(~ .metric, scales="free_y")

plot

# Show the accuracy for each fold for each model:
accuracy <- model_res %>% 
  filter(.metric == "accuracy") %>% 
  select(model_name, id, .estimate) %>% 
  pivot_wider(names_from = "model_name", values_from = ".estimate")

accuracy

# Show the roc_auc for each fold for each model:
roc_acu <- model_res %>% 
  filter(.metric == "roc_auc") %>% 
  select(model_name, id, .estimate) %>% 
  pivot_wider(names_from = "model_name", values_from = ".estimate")

roc_acu

# Visulaize the above:
plot1 <- model_res %>% 
  ggplot(aes(x=model_name, y= .estimate, color = id, group = id)) +
  geom_line() + 
  facet_wrap(~.metric, scales="free_y")

plot1

# We can see that there is not a lot of variability in the different folds. However there is quite a lot of variability between the models themselves.

# What is the density of the 3 models:
density_plot <- model_res %>% 
  ggplot(aes(x=.estimate, color = model_name, fill = model_name)) +
  geom_density(alpha = 0.1) + 
  facet_wrap(~.metric, scales="free")

density_plot

# What are the overall metrics for these three models.
metrics <- model_res %>% 
  group_by(model_name, .metric) %>% 
  summarise(mean=mean(.estimate))

metrics
```

# XGBOOST Model Tuning

```{r}

# Define an XGBoost model with hyperparameters set for tuning
xgb_spec <- boost_tree(trees = 1000, # Only have 1000 trees
                        tree_depth = tune(), #Maximum depth of the tree
                        min_n = tune(), #Minimum no. of data points in a leaf node
                        loss_reduction = tune(), #Minimum loss reduction (gamma)
                        sample_size = tune(), #Subsample ratio of the training data
                        mtry = tune(), #No. of features to consider at each split
                        learn_rate = tune()) %>% #Learning rate (eta)
  set_engine("xgboost", objective = "binary:logistic") %>% 
  set_mode("classification")

xgb_spec 

# Create a grid for hyperparameter tuning using a Latin hypercube design
xgb_grid <- grid_latin_hypercube(tree_depth(), #Maximum depth of the tree
                                  min_n(), #Min no. of data points in a leaf node
                                  loss_reduction(), #Minimum loss reduction (gamma)
                                  sample_size = sample_prop(), #Subsample ratio of the training
                                  finalize(mtry(), train_data),
                                  learn_rate(),
                                  size = 20) #Generate a set of 20 combinations of these hyperparameters for the tuning process
xgb_grid

# Create a workflow combining the recipe and the XGBoost model
xgb_wf <- workflow() %>% add_recipe(surv_recipe) %>% add_model(xgb_spec)

# Define 5-fold cross-validation with 3 repeats
cv_folds <- vfold_cv(train_data, v=5, repeats=3)

# Register parallel processing to speed up computation
doParallel::registerDoParallel()

# Set seed for reproducibility in tuning
set.seed(234)

# Tune the XGBoost model using cross-validation folds and the grid of hyperparameters
xgb_res <- tune_grid(xgb_wf, 
                     resamples = cv_folds, 
                     grid = xgb_grid,
                     control = control_grid(save_pred = TRUE))

# Collect and print the cross-validation results
xgb_res

# Explore the results
collect_metrics(xgb_res)

# Visualize the results, focusing on the ROC AUC metric
# xgb_res %>%
#   collect_metrics() %>%
#   filter(.metric == "roc_auc") %>%
#   select(mean, mtry:sample_size) %>%
#   pivot_longer(mtry:sample_size,
#                values_to = "value",
#                names_to = "parameter"
#   ) %>%
#   ggplot(aes(value, mean, color = parameter)) +
#   geom_point(alpha = 0.8, show.legend = FALSE) +
#   facet_wrap(~parameter, scales = "free_x") +
#   labs(x = NULL, y = "AUC")

# Show the best results based on ROC AUC
show_best(xgb_res)

# Select the best hyperparameters based on ROC AUC
best_auc <- select_best(xgb_res)
best_auc

# Finalize the workflow with the best hyperparameters
final_xgb <- finalize_workflow(xgb_wf, best_auc)
final_xgb

# Fit the finalized model on the training set and evaluate it on the testing set
final_res_xgb <- last_fit(final_xgb, data_split)

# Collect and print the evaluation metrics
collect_metrics(final_res_xgb)

# View predictions on the testing set
final_res_xgb %>% collect_predictions() %>% View()

# Plot the ROC curve for the final model's predictions
# final_res %>%
#   collect_predictions() %>%
#   mutate(pred_class_numeric = as.numeric(.pred_class)) %>%
#   roc_curve(truth=event, pred_class_numeric) %>%
#   ggplot(aes(x = 1 - specificity, y = sensitivity)) +
#   geom_line(size = 1.5, color = "midnightblue") +
#   geom_abline(
#     lty = 2, alpha = 0.5,
#     color = "gray50",
#     size = 1.2)

final_res_xgb %>% collect_predictions() %>% ggplot(aes(x=.pred_AD, fill=.pred_class)) + geom_density(alpha=0.5)

```

# Random Forest Model Tuning

```{r}

# Define the Random forest model with hyperparameters set for tuning, settting engine to 'ranger' provides a fast implementation of random forests for this classification task & Set mode to 'classification' for binary outcomes

# Define the Random forest model with hyperparameters set for tuning
rf_spec <- rand_forest(trees = 1000, #1000 trees in the forest
                       mtry = tune(), #No. of features to consider at each split
                       min_n = tune()) %>% #Min no. of data points in a leaf node
  set_engine("ranger") %>%
  set_mode("classification")

# Create a grid for hyperparameter tuning using a Latin hypercube design
rf_grid <- grid_latin_hypercube(min_n(), #Min no. of data points in a leaf node
                                finalize(mtry(), train_data),
                                size = 20) #Generate a set of 20 combinations of these hyperparameters for the tuning process


rf_grid

# Create a workflow combining the recipe and the XGBoost model
rf_wf <- workflow() %>% add_recipe(surv_recipe) %>% add_model(rf_spec)

# Define 5-fold cross-validation with 3 repeats
cv_folds <- vfold_cv(train_data, v=5, repeats=3)

# Register parallel processing to speed up computation
doParallel::registerDoParallel()

# Set seed for reproducibility in tuning
set.seed(234)

# Tune the XGBoost model using cross-validation folds and the grid of hyperparameters
rf_res <- tune_grid(rf_wf, 
                     resamples = cv_folds, 
                     grid = rf_grid,
                     control = control_grid(save_pred = TRUE))

# Collect and print the cross-validation results
rf_res

# Explore the results
collect_metrics(rf_res)

# Visualize the results, focusing on the ROC AUC metric
# rf_res %>%
#   collect_metrics() %>%
#   filter(.metric == "roc_auc") %>%
#   select(mean, mtry:sample_size) %>%
#   pivot_longer(mtry:sample_size,
#                values_to = "value",
#                names_to = "parameter"
#   ) %>%
#   ggplot(aes(value, mean, color = parameter)) +
#   geom_point(alpha = 0.8, show.legend = FALSE) +
#   facet_wrap(~parameter, scales = "free_x") +
#   labs(x = NULL, y = "AUC")

# Show the best results based on ROC AUC
show_best(rf_res)

# Select the best hyperparameters based on ROC AUC
best_auc <- select_best(rf_res)
best_auc

# Finalize the workflow with the best hyperparameters
final_rf <- finalize_workflow(rf_wf, best_auc)
final_rf

# Fit the finalized model on the training set and evaluate it on the testing set
final_res <- last_fit(final_rf, data_split)

# Collect and print the evaluation metrics
collect_metrics(final_res)

final_res %>%
  collect_predictions()%>%
  mutate(pred_class_numeric = 3 - as.numeric(.pred_class)) %>% View()

# Plot the ROC curve for the final model's predictions
# final_res %>%
#   collect_predictions() %>%
#   mutate(pred_class_numeric = 3 - as.numeric(.pred_class)) %>%
#   roc_curve(truth=event, pred_class_numeric) %>%
#   ggplot(aes(x = 1 - specificity, y = sensitivity)) +
#   geom_point(size = 1.5, color = "midnightblue") +
#   geom_abline(
#     lty = 2, alpha = 0.5,
#     color = "gray50",
#     size = 1.2)

final_res %>% collect_predictions() %>% ggplot(aes(x=.pred_AD, fill=.pred_class)) + geom_density(alpha=0.5)

```

# Logistic Regresion Model Tuning

```{r}

```
