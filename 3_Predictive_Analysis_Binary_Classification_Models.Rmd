---
title: "ADNI Data V2 - Binary Classification  Predictive Models"
output: html_document
date: "2023-03-29"
---

# 3_Predictive_Analysis_Binary_Classification_Models

## Install and load the relevant packages

```{r}

# install.packages("caret")
library(caret)

# install.packages("ranger")
library(ranger)

# install.packages("tidymodels")
library(tidymodels)

# install.packages("tidyverse")
library(tidyverse)

# install.packages("glmnet")
library(glmnet)

# install.packages("modeldatatoo")
library(modeldatatoo)

# install.packages("aorsf")
library(aorsf)

# install.packages("censored")
library(censored)

# install.packages("survival")
library(survival)

# install.packages("doParallel")
library(doParallel)

library(dplyr)

library(ggplot2)

library(gridExtra)
library(grid)

```

# SETUP

```{r}

# Import preprocessed data 
surv_df <- read.csv("preprocessed_data.csv")
#View(surv_df)View(surv_df)

# Convert the event column to a factor
surv_df$event <- factor(surv_df$event, levels = c(0, 1), labels = c("No AD", "AD"))

# Convert PTGENDER to numeric (0 for Male, 1 for Female)
surv_df <- surv_df %>%
  mutate(SEX = ifelse(SEX == "Male", 0, 1))

# Set seed for reproducibility
set.seed(123)

# Define resampling scheme: repeated k-fold cross-validation
cv_folds <- vfold_cv(surv_df, v = 5, repeats = 3)

# List of different recipes based on specified feature combinations

# D = Demographics 
# F = Family History
# DF = Demographics + Family History 
# DGF = Demographics + Genetics + Family History 
# DGCF = Demographics + Genetics + Cognition + Family History
# DGC = Demographics + Genetics + Cognition

recipes_list <- list(
  D = recipe(event ~ AGE + SEX + EDU, data = surv_df),
  Fh = recipe(event ~ FH, data = surv_df),
  DFh = recipe(event ~ AGE + SEX + EDU + FH, data = surv_df),
  DGFh = recipe(event ~ AGE + SEX + EDU + APOE4 + FH, data = surv_df),
  DGCFh = recipe(event ~ AGE + SEX + EDU + APOE4 + MMSE + mPACCtrailsB + FH, data = surv_df),
  DGC = recipe(event ~ APOE4 + AGE + SEX + EDU + MMSE + mPACCtrailsB, data = surv_df)
)


# Initialize list to store results for each recipe and model
results_list <- list()


# Define hyperparameter grids for Random Forest
rf_grid <- expand.grid(
  # trees = 1000,
  mtry = c(3, 5, 7),
  min_n = c(5, 10, 20)
)
# print(rf_grid)

# Define hyperparameter grids for XGBoost
xgb_grid <- expand.grid(
  # trees = 200,
  learn_rate = c(0.03, 0.08, 0.3, 0.5),
  tree_depth = c(2, 6)
)
# print(xgb_grid)
```

# TRAIN

```{r}

# Iterate through feature sets and run models for each
for (i in seq_along(recipes_list)) {
  current_recipe <- recipes_list[[i]]
  
  # Define models
  
  # Logistic Regression with alpha and l1_ratio tuning
  glm_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(logistic_reg() %>%
                set_engine("glm") %>%
                set_mode("classification"))

  # ## Tuned Random Forest
  rf_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(rand_forest(trees = 1000, 
                          mtry = tune(), 
                          min_n = tune()) %>%
                set_engine("ranger") %>%
                set_mode("classification"))

  # Tuned XGBoost
  xgb_wflow <- workflow() %>%
    add_recipe(current_recipe) %>%
    add_model(boost_tree(trees = 15, 
                         tree_depth = tune(), 
                         learn_rate = tune()) %>%
                set_engine("xgboost", objective = "binary:logistic") %>%
                set_mode("classification"))

  # Define evaluation metrics (ROC AUC)
  classification_metrics <- metric_set(accuracy, roc_auc, brier_class)

  # Fit models using resampling
  glm_res <- fit_resamples(glm_wflow, 
                           resamples = cv_folds, 
                           metrics = classification_metrics, 
                           control = control_resamples(save_pred = TRUE))
  
  rf_res <- tune_grid(rf_wflow, 
                      resamples = cv_folds, 
                      grid = rf_grid, 
                      metrics = classification_metrics, 
                      control = control_grid(save_pred = TRUE))
  
  xgb_res <- tune_grid(xgb_wflow, 
                       resamples = cv_folds, 
                       grid = xgb_grid, 
                       metrics = classification_metrics, 
                       control = control_grid(save_pred = TRUE))
  
  
  # Store the results for each model
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_Logistic")]] <- glm_res
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_RF")]] <- rf_res
  results_list[[paste0("Feature_Set_", names(recipes_list)[i], "_XGBoost")]] <- xgb_res
}

```

# PLOT

```{r}

# Function to extract and prepare metrics for plotting
prepare_plot_data <- function(result, model_name, feature_set_name) {
  collect_metrics(result, summarize = FALSE) %>%
    mutate(Model = model_name, feature_set = feature_set_name)
}

# Initialize an empty data frame to store all metrics
all_metricsv5 <- data.frame()
all_metricsv5

# Loop through results and extract metrics for each model and feature set
for (name in names(results_list)) {
  model_name <- str_extract(name, "Logistic|RF|XGBoost")
  feature_set_name <- str_extract(name, "(?<=Feature_Set_)[^_]+")  # Now use the name directly as 'Feature_Set_D', 'Feature_Set_DF', etc.
  print(feature_set_name)
  metrics_data <- prepare_plot_data(results_list[[name]], model_name, feature_set_name)
  all_metricsv5 <- bind_rows(all_metricsv5, metrics_data)
}


# Summarize metrics for plotting
summary_metrics <- all_metricsv5 %>%
  group_by(feature_set, Model, .metric)
summary_metrics
####################################

# Plot all metrics for all models and feature sets
BinaryAllModels_0 <- ggplot(summary_metrics, aes(x = feature_set, y = .estimate, fill = Model)) +
  geom_boxplot() +
  facet_wrap(~ .metric, scales = "free_y") +
  labs(title = "Binary Classification - Demographics Across All Models", 
       x = "Feature Set(s)", 
       y = "Metric Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 13) + 
  theme(legend.position = 'bottom')


# Print the box plot
print(BinaryAllModels_0)

# Save the updated plot
ggsave("Binary_All_Models_0.png", plot = BinaryAllModels_0, width = 10, height = 10)
```

# Reorder the recipes (worst performing model should be on the left and best on the right)

```{r}

# Calculate the mean value for each feature set and model
summary_metrics <- all_metricsv5 %>%
  group_by(feature_set, Model, .metric) %>%
  summarize(mean_estimate = mean(.estimate, na.rm = TRUE), .groups = "drop")

# Order feature sets by average ROC AUC (or your chosen metric)
best_feature_order <- summary_metrics %>%
  filter(.metric == "roc_auc") %>%      # Filter to use "roc_auc" or another metric for ordering
  arrange(mean_estimate) %>%            # Sort in ascending order for worst to best
  distinct(feature_set) %>%             # Remove duplicate levels if any
  pull(feature_set)

# Ensure the factor levels are unique in the desired order
all_metricsv5$feature_set <- factor(all_metricsv5$feature_set, levels = unique(best_feature_order))

# Plot with ordered feature sets on x-axis
Binaryglm_All_ordered <- ggplot(all_metricsv5, aes(x = feature_set, y = .estimate, fill = Model)) +
  geom_boxplot() +
  facet_wrap(~ .metric, scales = "free_y") +
  labs(title = "Binary Classification - Comparison of Feature Sets Across Best Model", 
       x = "Feature Set(s)", 
       y = "Metric Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 13) + 
  theme(legend.position = 'bottom')

# Print the box plot
print(Binaryglm_All_ordered)

# Save the updated plot
ggsave("Binary_All_ordered.png", plot = Binaryglm_All_ordered, width = 10, height = 10)
```

# Print & save only the AUC ROC

```{r}
# Filter for AUC-ROC metric only
roc_auc_metrics <- all_metricsv5 %>%
  filter(.metric == "roc_auc")

# Reorder feature sets based on the average AUC-ROC, from worst to best
best_feature_order <- roc_auc_metrics %>%
  group_by(feature_set) %>%
  summarize(mean_auc = mean(.estimate, na.rm = TRUE)) %>%
  arrange(mean_auc) %>%
  pull(feature_set)

# Ensure feature sets appear in the desired order on the x-axis
roc_auc_metrics$feature_set <- factor(roc_auc_metrics$feature_set, levels = unique(best_feature_order))

# Plot ROC AUC for each feature set, ordered on the x-axis
Binaryglm_AUC_Only <- ggplot(roc_auc_metrics, aes(x = feature_set, y = .estimate, fill = Model)) +
  geom_boxplot() +
  labs(title = "Binary Classification - Comparison Across Feature Sets", 
       x = "Feature Set(s)", 
       y = "AUC-ROC") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  theme_light(base_size = 25) + 
  theme(legend.position = 'bottom')

# Print the AUC-ROC box plot
print(Binaryglm_AUC_Only)

# Save the updated plot
ggsave("Binaryglm_AUC_Only.png", plot = Binaryglm_AUC_Only, width = 10, height = 10)
```

# TABLE

```{r}

# First, collect the best tuning parameters for Random Forest and XGBoost
# This will give you the best parameter values for each fold and repeat combination

# For Random Forest (rf_res)
rf_best_params <- rf_res %>%
  select_best(metric = "roc_auc") %>%
  mutate(Model = "RF")

# For XGBoost (xgb_res)
xgb_best_params <- xgb_res %>%
  select_best(metric = "roc_auc") %>%
  mutate(Model = "XGBoost")

# Combine all the best tuning parameters together
# Adding in the columns 'min_n', 'mtry', 'tree_depth', 'learn_rate' where applicable
best_params <- bind_rows(rf_best_params, xgb_best_params)

# Now, calculate the summary statistics (Median, Q1, Q3) for each feature set and model
summary_metrics <- all_metricsv5 %>%
  group_by(feature_set, Model, .metric) %>%
  summarize(
    Median = median(.estimate, na.rm = TRUE),
    Q1 = quantile(.estimate, 0.25, na.rm = TRUE),
    Q3 = quantile(.estimate, 0.75, na.rm = TRUE)
  )

# Join the summary statistics with the best tuning parameters
final_summary <- summary_metrics %>%
  left_join(best_params, by = "Model")
# #View the final table
# print(final_summary)
 
# Filter for rows where .metric == "roc_auc" and round the numeric columns to 3 decimal places
roc_auc_summary <- final_summary %>%
  filter(.metric == "roc_auc") %>%
  mutate(across(c(Median, Q1, Q3), round, 3))  # Round Median, Q1, Q3 to 3 decimal places

# Print the filtered summary table
print(roc_auc_summary)

# Save the final table with median, quartiles, and tuning parameters as a CSV
write.csv(final_summary, "summary_metrics_with_tuning_params.csv", row.names = FALSE)

```

# External Validation on UKBB & AIBL

```{r}

# Import preprocessed data 
ADNI_subset <- read.csv("ADNI_subset.csv")

# View(ADNI_subset)

AIBL_subset <- read.csv("AIBL_subset.csv")
# View(AIBL_subset)

UKBB_subset <- read.csv("UKBB_subset.csv")
# View(UKBB_subset)

# Import preprocessed data 
surv_df <- read.csv("preprocessed_data.csv")
# View(surv_df)

# Convert the event column to a factor
UKBB_subset$conv <- factor(UKBB_subset$conv, levels = c(0, 1), labels = c("No AD", "AD"))

# Convert the event column to a factor
AIBL_subset$conv <- factor(AIBL_subset$conv, levels = c(0, 1), labels = c("No AD", "AD"))

# Convert the event column to a factor
surv_df$event <- factor(surv_df$event, levels = c(0, 1), labels = c("No AD", "AD"))

# Convert PTGENDER to numeric (0 for Male, 1 for Female)
surv_df <- surv_df %>%
  mutate(SEX = ifelse(SEX == "Male", 0, 1))


```

```{r}


library(tidymodels)
library(dplyr)
library(broom)

# List of recipes
recipes_list <- list(
  D = recipe(event ~ AGE + SEX + EDU, data = surv_df),
  Fh = recipe(event ~ FH, data = surv_df),
  DFh = recipe(event ~ AGE + SEX + EDU + FH, data = surv_df),
  DGFh = recipe(event ~ AGE + SEX + EDU + APOE4 + FH, data = surv_df)
)

# Define XGBoost hyperparameters
xgb_spec <- boost_tree(trees = 15, tree_depth = 2, learn_rate = 0.3) %>%
  set_engine("xgboost", objective = "binary:logistic") %>%
  set_mode("classification")

# Logistic Regression model specification
glm_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# # List of final models
# models_list <- list(
#   D_xgb = workflow() %>% add_recipe(recipes_list[["D"]]) %>% add_model(xgb_spec),
#   Fh_glm = workflow() %>% add_recipe(recipes_list[["Fh"]]) %>% add_model(glm_spec),
#   DFh_xgb = workflow() %>% add_recipe(recipes_list[["DFh"]]) %>% add_model(xgb_spec),
#   DGFh_glm = workflow() %>% add_recipe(recipes_list[["DGFh"]]) %>% add_model(glm_spec)
# )
# 
# List of final models
models_list <- list(
  D_glm = workflow() %>% add_recipe(recipes_list[["D"]]) %>% add_model(glm_spec),
  Fh_glm = workflow() %>% add_recipe(recipes_list[["Fh"]]) %>% add_model(glm_spec),
  DFh_glm = workflow() %>% add_recipe(recipes_list[["DFh"]]) %>% add_model(glm_spec),
  DGFh_glm = workflow() %>% add_recipe(recipes_list[["DGFh"]]) %>% add_model(glm_spec)
)


# # List of final models
# models_list <- list(
#   D_xgb = workflow() %>% add_recipe(recipes_list[["D"]]) %>% add_model(xgb_spec),
#   Fh_xgb = workflow() %>% add_recipe(recipes_list[["Fh"]]) %>% add_model(xgb_spec),
#   DFh_xgb = workflow() %>% add_recipe(recipes_list[["DFh"]]) %>% add_model(xgb_spec),
#   DGFh_xgb = workflow() %>% add_recipe(recipes_list[["DGFh"]]) %>% add_model(xgb_spec)
# )


# Train all models on ADNI dataset
fit_models <- lapply(models_list, function(model_wflow) {
  model_wflow %>% fit(data = surv_df)
})

# List to store ROC AUC results
roc_auc_results <- data.frame(
  Model = character(),
  Train_ROC_AUC = numeric(),
  UKBB_ROC_AUC = numeric(),
  AIBL_ROC_AUC = numeric(),
  stringsAsFactors = FALSE
)

# Evaluate all models on the ADNI training dataset, UKBB subset, and AIBL subset
for (model_name in names(fit_models)) {
  fit <- fit_models[[model_name]]
  
  # Get predictions on ADNI dataset (training set)
  preds_train <- predict(fit, new_data = surv_df, type = "prob") %>%
    bind_cols(predict(fit, new_data = surv_df)) %>%
    bind_cols(surv_df)
  
  # Store the ROC AUC metric for training set
  roc_auc_train <- preds_train %>%
    roc_auc(truth = event, '.pred_No AD')  # `event` is the target column in surv_df

  
  # Get predictions on UKBB dataset
  preds_ukbb <- predict(fit, new_data = UKBB_subset, type = "prob") %>%
    bind_cols(predict(fit, new_data = UKBB_subset)) %>%
    bind_cols(UKBB_subset)
  
  # Store the ROC AUC metric for UKBB
  roc_auc_ukbb <- preds_ukbb %>%
    roc_auc(truth = conv, '.pred_No AD')

  # Get predictions on AIBL dataset
  preds_aibl <- predict(fit, new_data = AIBL_subset, type = "prob") %>%
    bind_cols(predict(fit, new_data = AIBL_subset)) %>%
    bind_cols(AIBL_subset)
  
  # Store the ROC AUC metric for AIBL
  roc_auc_aibl <- preds_aibl %>%
    roc_auc(truth = conv, '.pred_No AD')

  # Save the results rounded to 3 decimal places
  roc_auc_results <- roc_auc_results %>%
    add_row(
      Model = model_name,
      Train_ROC_AUC = round(roc_auc_train$.estimate, 3),
      UKBB_ROC_AUC = round(roc_auc_ukbb$.estimate, 3),
      AIBL_ROC_AUC = round(roc_auc_aibl$.estimate, 3)
    )
}

# #View results
roc_auc_results

```

```{r}

library(tidymodels)
library(dplyr)
library(broom)

# Copy the original datasets to add predictions to
updated_UKBB_subset <- UKBB_subset
updated_AIBL_subset <- AIBL_subset

# Loop over each model and get predictions
for (model_name in names(fit_models)) {
  fit <- fit_models[[model_name]]
  
  # Generate predictions (class labels) for each subset and rename columns
  preds_ukbb <- predict(fit, new_data = UKBB_subset) %>% pull(.pred_class)
  preds_aibl <- predict(fit, new_data = AIBL_subset) %>% pull(.pred_class)
  
  # Add predictions as new columns in each updated dataset
  updated_UKBB_subset[[model_name]] <- preds_ukbb
  updated_AIBL_subset[[model_name]] <- preds_aibl
}

# View updated datasets with predictions
View(updated_UKBB_subset)
View(updated_AIBL_subset)


```
